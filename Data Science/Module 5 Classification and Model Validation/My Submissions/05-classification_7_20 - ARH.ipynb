{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snMo9ZsfZrjJ"
   },
   "source": [
    "## Project 5 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD5coBt7ZrjM"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Practice classification on the Titanic dataset.\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy1IWSV2ZrjN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# import sklearn.datasets\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from pandas import DataFrame\n",
    "# from scipy.stats import norm\n",
    "# from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "yTgZ8xwPZrjO"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "On April 15, 1912, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck resulted in such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\n",
    "\n",
    "Intro Videos: \n",
    "https://www.youtube.com/watch?v=3lyiZMeTKIo\n",
    "and\n",
    "https://www.youtube.com/watch?v=ItjXTieWKyI \n",
    "\n",
    "The `titanic_data.csv` file contains data for `887` of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the person including whether they survived (`0=No`), their age, their passenger-class (`1=1st Class, Upper`), gender, and the fare they paid (Â£s*). For more on the currency: http://www.statisticalconsultants.co.nz/blog/titanic-fare-data.html\n",
    "\n",
    "We are going to try to see if there are correlations between the feature data provided (find a best subset of features) and passenger survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo2wAo3vZrjO"
   },
   "source": [
    "### Problem 1: Load and understand the data (35 points)\n",
    "\n",
    "#### Your task (some of this is the work you completed for L14 - be sure to copy that work into here as needed)\n",
    "Conduct some preprocessing steps to explore the following and provide code/answers in the below cells:\n",
    "1. Load the `titanic_data.csv` file into a pandas dataframe\n",
    "2. Explore the data provided (e.g., looking at statistics using describe(), value_counts(), histograms, scatter plots of various features, etc.) \n",
    "3. What are the names of feature columns that appear to be usable for learning?\n",
    "4. What is the name of the column that appears to represent our target?\n",
    "5. Formulate a hypothesis about the relationship between given feature data and the target\n",
    "6. How did Pclass affect passenngers' chances of survival?\n",
    "7. What is the age distribution of survivors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat titanic_data.csv\n",
    "# pd.read_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_FdHGWgZrjP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1. Load the `titanic_data.csv` file into a pandas dataframe\n",
    "titn = pd.read_table('titanic_data.csv', sep=',',index_col='Name')\n",
    "titn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W39VMl34ZrjP"
   },
   "outputs": [],
   "source": [
    "# Step 2. Explore the data provided (e.g., looking at statistics using describe(), value_counts(), histograms, scatter plots of various features, etc.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_titn_dummies = pd.get_dummies(titn,columns=['Sex'])\n",
    "# print(df_titn_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_0 = titn[titn['Survived'] == 0]\n",
    "survived_1 = titn[titn['Survived'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "columns = survived_1.columns\n",
    "\n",
    "# Create figure with subplots and change the size to look good\n",
    "fig, axs = plt.subplots(4, 2, figsize=(10, 15))\n",
    "\n",
    "# Iterate through each column\n",
    "for i, f in enumerate(columns):\n",
    "    row = i // 2 #determining what row the plot goes in\n",
    "    col = i % 2 #determining the column by calculating the remainder\n",
    "    \n",
    "    # Plot histograms for survived_1 and _0, alpha is 50% so that we can see the data overlap\n",
    "    axs[row, col].hist(survived_1[f], alpha=0.5, label='Lived')\n",
    "    axs[row, col].hist(survived_0[f], alpha=0.5, label='Died')\n",
    "    axs[row, col].set_xlabel(f)\n",
    "    axs[row, col].set_ylabel('Count')\n",
    "    axs[row, col].set_title(f)\n",
    "    axs[row, col].legend()\n",
    "\n",
    "#make it tight!\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This was my test code, I then combined it with above\n",
    "# # Calculate how many survived from each\n",
    "# age_cnt_died = survived_0['Age'].value_counts().sort_index()\n",
    "# age_cnt_lived = survived_1['Age'].value_counts().sort_index()\n",
    "\n",
    "# # Calculate total count for each age group\n",
    "# tot_cnt = age_cnt_died + age_cnt_lived\n",
    "\n",
    "# # Calculate the percentage of individuals who survived for each age group\n",
    "# perct_survived = (age_cnt_lived / tot_cnt) * 100\n",
    "\n",
    "# # Create a scatter plot\n",
    "# plt.scatter(perct_survived.index, perct_survived.values)\n",
    "# plt.xlabel('Age')\n",
    "# plt.ylabel('Percentage Survived')\n",
    "# plt.title('Percentage Survived by Age')\n",
    "# plt.ylim(0,100)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "columns = survived_1.columns\n",
    "\n",
    "# Create figure with subplots and change the size to look good\n",
    "fig, axs = plt.subplots(4, 2, figsize=(10, 15))\n",
    "\n",
    "# Iterate through each column\n",
    "for i, f in enumerate(columns):\n",
    "    row = i // 2 #determining what row the plot goes in\n",
    "    col = i % 2 #determining the column by calculating the remainder\n",
    "    \n",
    "    # Calculate how many survived from each\n",
    "    f_cnt_died = survived_0[f].value_counts().sort_index()\n",
    "    f_cnt_lived = survived_1[f].value_counts().sort_index()\n",
    "\n",
    "    # Calculate total count for each group\n",
    "    tot_cnt = f_cnt_died + f_cnt_lived\n",
    "\n",
    "    # Calculate the percentage of individuals who survived for each group\n",
    "    perct_survived = (f_cnt_lived / tot_cnt) * 100\n",
    "    \n",
    "    # Plot scatters\n",
    "    axs[row, col].bar(perct_survived.index, perct_survived.values)\n",
    "    axs[row, col].set_xlabel(f)\n",
    "    axs[row, col].set_ylabel('Percentage Survived')\n",
    "    axs[row, col].set_title(f)\n",
    "    axs[row, col].set_ylim(0,100)\n",
    "\n",
    "#make it tight!\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate how many survived from each fare group\n",
    "f_cnt_died = survived_0.groupby('Pclass')['Fare'].value_counts().sort_index()\n",
    "f_cnt_lived = survived_1.groupby('Pclass')['Fare'].value_counts().sort_index()\n",
    "\n",
    "# Calculate total count for each fare group\n",
    "tot_cnt = f_cnt_died + f_cnt_lived\n",
    "\n",
    "# Calculate the percentage of individuals who survived for each fare group\n",
    "perct_survived = (f_cnt_lived / tot_cnt) * 100\n",
    "\n",
    "# Assign colors based on Pclass\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "# Go through each Pclass and plot the percentage of fare survived\n",
    "for i, pclass in enumerate(perct_survived.index.levels[0]):\n",
    "    plt.scatter(perct_survived[pclass].index, perct_survived[pclass], label=f'Pclass {pclass}', color=colors[i], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Percentage Survived')\n",
    "plt.title('Percentage Survived by Fare and Pclass')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P3MyrYvZrjP"
   },
   "source": [
    "---\n",
    "\n",
    "**Edit this cell to provide answers to the following steps:**\n",
    "\n",
    "---\n",
    "\n",
    "Step 3. What are the names of feature columns that appear to be usable for learning?\n",
    "\n",
    "<font color = blue>\n",
    "    \n",
    "Survived (as the target), Pclass, Sex, Age, Fare all appear useful. There are some interesting trends in Siblings/Spouse and Childre/Parents data, but I am not certain how useful they will be. Maybe if we combine that information into #of family members on board? That might be more useful. \n",
    "    \n",
    "</font>\n",
    "\n",
    "Step 4. What is the name of the column that appears to represent our target?\n",
    "\n",
    "<font color = blue>\n",
    "\n",
    "Survived\n",
    "    \n",
    "</font>\n",
    "\n",
    "Step 5. Formulate a hypothesis about the relationship between given feature data and the target\n",
    "\n",
    "<font color = blue>\n",
    "\n",
    "I believe there will be stronger correlations between gender and surviving, and possibly a skew in the data for age (those younger than 20) surviving. Per the age old \"women and children\" first. I also suspect to see a higher percentage of 1st class passengers surviving.\n",
    "    \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "UT2-1I86ZrjQ"
   },
   "source": [
    "Step 6. How did Pclass affect passenngers' chances of survival?\n",
    "Show your work with a bar plot, dataframe selection, or visual of your choice.\n",
    "\n",
    "<font color = blue> \n",
    "    \n",
    "From the plot below we can see that a higher class meant a higher likelyhood to survive. I like percentages for visualization. If you were first class roughly 60% chance, second 40-50%, and third almost 20%. These values aren't exact, (I could have printed them if we wanted). But, I think the trend is clear here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Took my test code and changed it for Pclass, this graph is actually already found above, but here it is alone\n",
    "# Calculate how many survived from each\n",
    "P_cnt_died = survived_0['Pclass'].value_counts().sort_index()\n",
    "P_cnt_lived = survived_1['Pclass'].value_counts().sort_index()\n",
    "\n",
    "# Calculate total count for each age group\n",
    "tot_cnt = P_cnt_died + P_cnt_lived\n",
    "\n",
    "# Calculate the percentage of individuals who survived for each age group\n",
    "perct_survived = (P_cnt_lived / tot_cnt) * 100\n",
    "\n",
    "# # Create a bar plot\n",
    "plt.bar(perct_survived.index, perct_survived.values)\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('Percentage Survived')\n",
    "plt.title('Percentage Survived by Class')\n",
    "plt.ylim(0,100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "beX7g2S2ZrjQ"
   },
   "source": [
    "Step 7. What is the age distribution of survivors?\n",
    "Show your work with a dataframe operation and/or histogram plot.\n",
    "\n",
    "<font color = blue>\n",
    "    \n",
    "See the plot below, but we can see that most people are between the ages of 15 and 40. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(titn['Age'], bins=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei9hTK4wZrjR"
   },
   "source": [
    "### Problem 2: transform the data (10 points)\n",
    "The `Sex` column is categorical, meaning its data are separable into groups, but not numerical. To be able to work with this data, we need numbers, so you task is to transform the `Sex` column into numerical data with pandas' `get_dummies` feature and remove the original categorical `Sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcBPWipsZrjR"
   },
   "outputs": [],
   "source": [
    "df_titn_dum=pd.get_dummies(titn,columns=['Sex'])\n",
    "df_titn_dum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XipNK7BMZrjR"
   },
   "source": [
    "### Problem 3: Classification (30 points)\n",
    "Now that the data is transformed, we want to run various classification experiments on it. The first is `K Nearest Neighbors`, which you will conduct by:\n",
    "\n",
    "1. Define input and target data by creating lists of dataframe columns (e.g., inputs = ['Pclass', etc.)\n",
    "2. Split the data into training and testing sets with `train_test_split()`\n",
    "3. Create a `KNeighborsClassifier` using `5` neighbors at first (you can experiment with this parameter)\n",
    "4. Train your model by passing the training dataset to `fit()`\n",
    "5. Calculate predicted target values(y_hat) by passing the testing dataset to `predict()`\n",
    "6. Print the accuracy of the model with `score()`\n",
    "\n",
    "** Note: If you get a python warning as you use the Y, trainY, or testY vector in some of the function calls about \"DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, )\", you can look up how to use trainY.values.ravel() or trainY.values.flatten() or another function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titn_dum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbb25y7RZrjS"
   },
   "outputs": [],
   "source": [
    "inputs = df_titn_dum[['Pclass', 'Age', 'Fare', 'Sex_female', 'Sex_male']]\n",
    "target = df_titn_dum['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pS7mNB05ZrjS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = inputs\n",
    "y = target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# train, test = train_test_split(data, test_size = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk-xmVtEZrjS"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "model = neighbors.KNeighborsClassifier(k)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "# print(conf_matrix)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9prt7aTZrjS"
   },
   "source": [
    "### Problem 4: Cross validation, classification report (15 points)\n",
    "- Using the concepts from the 17-model_selection slides and the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function from scikit-learn, estimate the f-score ([`f1-score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) (you can use however many folds you wish). To get `cross_val_score` to use `f1-score` rather than the default accuracy measure, you will need to set the `scoring` parameter and use a scorer object created via [`make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer).  Since this has a few parts to it, let me just give you that parameter: ```scorerVar = make_scorer(f1_score, pos_label=1)```\n",
    "\n",
    "- Using the concepts from the end of the 14-classification slides, output a confusion matrix.\n",
    "\n",
    "- Also, output a classification report [`classification_report`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) from sklearn.metrics showing more of the metrics: precision, recall, f1-score for both of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud5y-XedZrjT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_test, y_pred, *, labels=None, pos_label=1, \n",
    "#                          average='binary', sample_weight=None, zero_division='warn')\n",
    "\n",
    "f1_score(y_test, y_pred)\n",
    "\n",
    "scorerVar = make_scorer(f1_score, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=scorerVar)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets_out = metrics.classification_report(y_test, y_pred)\n",
    "print(mets_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KRHtfpZZrjT"
   },
   "source": [
    "### Problem 5: Support Vector Machines (15 points)\n",
    "Now, repeat the above experiment using the using a Support Vector classifier [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) with default parameters (RBF kernel) model in scikit-learn, and output:\n",
    "\n",
    "- The fit accuracy (using the `score` method of the model)\n",
    "- The f-score (using the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function)\n",
    "- The confusion matrix\n",
    "- The precision, recall, and f-measure for the 1 class (you can just print the results of the [`classification_report`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function from sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGjGEZD6ZrjT"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#create a model object\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "#train our model\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#evaluate the model \n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#setup to get f-score and cv\n",
    "\n",
    "f1_score(y_test, y_pred)\n",
    "\n",
    "scorerVar = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=scorerVar)\n",
    "\n",
    "#confusion matrix\n",
    "\n",
    "conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "#classification report\n",
    "\n",
    "mets_out = metrics.classification_report(y_test, y_pred)\n",
    "print(mets_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSDsdQFOZrjU"
   },
   "source": [
    "### Problem 6: Logistic Regression (15 points)\n",
    "\n",
    "Now, repeat the above experiment using the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model in scikit-learn, and output:\n",
    "\n",
    "- The fit accuracy (using the `score` method of the model)\n",
    "- The f-score (using the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function)\n",
    "- The confusion matrix\n",
    "- The precision, recall, and f-measure for the 1 class (you can just print the results of the [`classification_report`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function from sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZIwTdPZrjU"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "#create a model object\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "#train our model\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#evaluate the model \n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#setup to get f-score and cv\n",
    "\n",
    "f1_score(y_test, y_pred)\n",
    "\n",
    "scorerVar = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=scorerVar)\n",
    "\n",
    "#confusion matrix\n",
    "\n",
    "conf_matrix = sk.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "#classification report\n",
    "\n",
    "mets_out = metrics.classification_report(y_test, y_pred)\n",
    "print(mets_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWmwls34ZrjU"
   },
   "source": [
    "### Problem 7: Comparision and Discussion (5 points)\n",
    "Edit this cell to provide a brief discussion (3-5 sentances at most):\n",
    "1. What was the model/algorithm that performed best for you?\n",
    "\n",
    "<font color = blue>Logistic Regression if we go off F-score. Both scores for surviving and not surviving were the highest, respectively, amongst all three models. </font>\n",
    "\n",
    "2. What feaures and parameters were used to achieve that performance?\n",
    "\n",
    "<font color = blue>Features: 'Pclass', 'Age', 'Fare', 'Sex_female', 'Sex_male'\n",
    "\n",
    "Parameters: splitting the data 75/25 for Train/Test, using a dictated randomization (state 42), and the different models</font>\n",
    "\n",
    "\n",
    "3. What insights did you gain from your experimentation about the predictive power of this dataset and did it match your original hypothesis about the relationship between given feature data and the target?\n",
    "\n",
    "<font color = blue>I am not certain. The F-factor is not as high as I would want it to be if I was using a model for myself (I would like something over 0.80). Perhaps the features  I left out (family members: Siblings/Spouse, Childre/Parents) did have some impact. I could rerun all of this with those features to see if that impacts the results. However, we can say that with F-scores around 0.7, there is some correlation. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZLuk1FmZrjU"
   },
   "source": [
    "### Questionnaire\n",
    "1) How long did you spend on this assignment? <font color = blue>5 hours</font>\n",
    "<br><br>\n",
    "2) What did you like about it? What did you not like about it? <font color = blue>This really challenged me to learn how these models work. It was stressful, because I am still not certain if it worked properly, but it feels like they did. Also, I liked the callback to previous assignments to keep that stuff fresh in my mind.</font>\n",
    "<br><br>\n",
    "3) Did you find any errors or is there anything you would like changed? <font color = blue>Not sure.</font>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQjugad_ZrjV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6qUjQrnZrjV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "05-classification_7_20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
